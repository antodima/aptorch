{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0850ca0b",
   "metadata": {},
   "source": [
    "# ESN trained on Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as L\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, Callback\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "from aptorch.data import get_shakespeare_text, CharacterDataset\n",
    "from aptorch.esn import ESN_Pretrained\n",
    "\n",
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c919479",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_shakespeare_text()\n",
    "train_size = int(len(text) * 0.9)\n",
    "train_text = text[:train_size]\n",
    "test_text = text[train_size:]\n",
    "characters = sorted(list(set(train_text)))\n",
    "pad_token, mask_token, unk_token = \"[PAD]\", \"[MASK]\", \"[UNK]\"\n",
    "special_tokens = {\n",
    "    0: pad_token,\n",
    "    1: mask_token,\n",
    "    2: unk_token,\n",
    "}\n",
    "start_idx = max(list(special_tokens.keys())) + 1\n",
    "char_tokens = {i + start_idx: c for i, c in enumerate(characters)}\n",
    "i2c = {**special_tokens, **char_tokens}  # index to char\n",
    "c2i = {v: k for k, v in i2c.items()}  # char to index\n",
    "print(f\"characters: {len(characters)}\")\n",
    "print(f\"i2c: {i2c}\")\n",
    "print(f\"c2i: {c2i}\")\n",
    "\n",
    "\n",
    "design_set = CharacterDataset(\n",
    "    text=train_text,\n",
    "    chat_to_token=c2i,\n",
    "    unk_token=unk_token,\n",
    ")\n",
    "test_set = CharacterDataset(\n",
    "    text=test_text,\n",
    "    chat_to_token=c2i,\n",
    "    unk_token=unk_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetState(Callback):\n",
    "    def on_epoch_end(self, trainer, pl_module: ESN_Pretrained):\n",
    "        pl_module.reservoir.reset_state()\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    params = {\n",
    "        \"input_size\": design_set.input_len,\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [8, 16, 32]),\n",
    "        \"num_tokens\": design_set.vocab_size,\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-3, 1e-1),\n",
    "    }\n",
    "\n",
    "    train_set, valid_set = random_split(design_set, [0.8, 0.2])\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        num_workers=9,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_set,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        num_workers=9,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    model = ESN_Pretrained(**params)\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\"val_loss\", min_delta=0.01)\n",
    "    priuning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")\n",
    "    reset_state_callback = ResetState()\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        max_epochs=10,\n",
    "        deterministic=True,\n",
    "        callbacks=[\n",
    "            early_stopping_callback,\n",
    "            priuning_callback,\n",
    "            reset_state_callback,\n",
    "        ],\n",
    "        # fast_dev_run=True,\n",
    "    )\n",
    "    trainer.logger.log_hyperparams(params)\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=valid_loader,\n",
    "    )\n",
    "    priuning_callback.check_pruned()\n",
    "\n",
    "    return trainer.callback_metrics[\"val_acc\"].item()\n",
    "\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(\n",
    "    study_name=\"esn_gpt\",\n",
    "    direction=\"maximize\",\n",
    "    pruner=pruner,\n",
    "    load_if_exists=False,\n",
    ")\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial: {study.best_trial}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
