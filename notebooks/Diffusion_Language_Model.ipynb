{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb50881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/aptorch-DwoBsKDE-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from aptorch.data import DivinaCommediaDataset, divina_commedia\n",
    "from aptorch.dlm import DLM, llada_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2829bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = divina_commedia()\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(pad_token=\"[PAD]\", pad_id=0)\n",
    "tokenizer.add_special_tokens([\"[PAD]\", \"[UNK]\", \"[MASK]\"])\n",
    "trainer = BpeTrainer()\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    train_dataset[\"text\"],\n",
    "    trainer=trainer,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.tensor([enc.ids for enc in tokenizer.encode_batch(batch)])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_set = DivinaCommediaDataset(dataset=train_dataset)\n",
    "test_set = DivinaCommediaDataset(dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb6d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    lr: float,\n",
    "    n_epochs: int,\n",
    "    batch_size: int,\n",
    "    emb_dim: int,\n",
    "    ff_dim: int,\n",
    "    mask_ratio: float,\n",
    "    pad_idx: int,\n",
    "    mask_idx: int,\n",
    "    num_tokens: int,\n",
    "):\n",
    "    torch.manual_seed(23)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loader = DataLoader(\n",
    "            train_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.\n",
    "        for i, x in enumerate(pbar := tqdm(train_loader)):\n",
    "            optim.zero_grad()\n",
    "            logits, mask = model(x, mask_ratio)\n",
    "\n",
    "            loss = torch.tensor(0.0)\n",
    "            if mask.sum() != 0:\n",
    "                loss = llada_loss(x, logits, mask) / mask_ratio\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                running_loss += loss.item()\n",
    "                pbar.set_description(\n",
    "                    f\"epoch {epoch+1}/{n_epochs}: loss={running_loss/(i+1):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "emb_dim = 32\n",
    "ff_dim = 512\n",
    "mask_ratio = random.uniform(0.01, 0.99)\n",
    "print(f\"mask_ratio={mask_ratio}\")\n",
    "pad_token_id = (tokenizer.encode(\"[PAD]\").ids)[0]\n",
    "mask_token_id = (tokenizer.encode(\"[MASK]\").ids)[0]\n",
    "num_tokens = tokenizer.get_vocab_size()\n",
    "\n",
    "model = DLM(\n",
    "    num_tokens=num_tokens,\n",
    "    emb_dim=emb_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    pad_idx=pad_token_id,\n",
    "    mask_idx=mask_token_id,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "print(\n",
    "    f\"Number of parameters={sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    lr=lr,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    emb_dim=emb_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    mask_ratio=mask_ratio,\n",
    "    pad_idx=pad_token_id,\n",
    "    mask_idx=mask_token_id,\n",
    "    num_tokens=num_tokens,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptorch-DwoBsKDE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
